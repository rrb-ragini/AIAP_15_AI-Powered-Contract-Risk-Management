{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Investigation Notebook\n",
                "\n",
                "This notebook sets up interactions with Gemini, ChatGPT, and Claude APIs.\n",
                "It assumes a `.env` file is present in the same directory with valid API keys."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "13a38cfd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Warning: One or more API keys are missing in the .env file.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from google import genai\n",
                "from openai import OpenAI\n",
                "import anthropic\n",
                "from typing import Optional, Literal, List\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
                "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
                "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
                "\n",
                "if not all([GEMINI_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY]):\n",
                "    print(\"Warning: One or more API keys are missing in the .env file.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddb3543d",
            "metadata": {},
            "source": [
                "## Configuration & Model Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "defa6417",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Configurations\n",
                "# You can change these to other available models as needed.\n",
                "GEMINI_MODEL = \"gemini-1.5-pro-latest\" # Or \"gemini-1.5-flash\"\n",
                "GPT_MODEL = \"gpt-4o\"\n",
                "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
                "\n",
                "# Initialize Clients\n",
                "if GEMINI_API_KEY:\n",
                "    genai.configure(api_key=GEMINI_API_KEY)\n",
                "\n",
                "if OPENAI_API_KEY:\n",
                "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
                "\n",
                "if ANTHROPIC_API_KEY:\n",
                "    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
                "\n",
                "# Define Pydantic Model for Structured Output\n",
                "class ClauseAnalysis(BaseModel):\n",
                "    clause_text: str = Field(..., description=\"The exact clause text identified from the contract\")\n",
                "    is_identified: bool = Field(..., description=\"Whether the clause falls into a known Golden Clause category\")\n",
                "    category: Optional[str] = Field(None, description=\"The Golden Clause category if identified, e.g., 'Limitation of Liability'\")\n",
                "    risk_rating: Literal[\"Low\", \"Medium\", \"High\"] = Field(..., description=\"The risk rating associated with the clause\")\n",
                "    justification: str = Field(..., description=\"Brief justification for the risk rating\")\n",
                "\n",
                "def call_llm(provider, prompt, system_instruction=None, response_model=None):\n",
                "    \"\"\"\n",
                "    Unified wrapper to call different LLM providers with structured output support.\n",
                "    provider: 'gemini', 'openai', or 'claude'\n",
                "    prompt: The user prompt.\n",
                "    system_instruction: Optional system instruction (supported differently by models).\n",
                "    response_model: Optional Pydantic model for structured output.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        if provider == \"gemini\":\n",
                "            # Note: system_instruction usage depends on the library version, mostly supported in recent versions.\n",
                "            generation_config = {}\n",
                "            if response_model:\n",
                "                generation_config[\"response_mime_type\"] = \"application/json\"\n",
                "                generation_config[\"response_schema\"] = response_model\n",
                "\n",
                "            model = genai.GenerativeModel(\n",
                "                GEMINI_MODEL, \n",
                "                system_instruction=system_instruction,\n",
                "                generation_config=generation_config\n",
                "            )\n",
                "            response = model.generate_content(prompt)\n",
                "            if response_model:\n",
                "                 # Gemini returns JSON text, we validate it against the model\n",
                "                 return response_model.model_validate_json(response.text)\n",
                "            return response.text\n",
                "\n",
                "        elif provider == \"openai\":\n",
                "            messages = []\n",
                "            if system_instruction:\n",
                "                messages.append({\"role\": \"system\", \"content\": system_instruction})\n",
                "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
                "            \n",
                "            if response_model:\n",
                "                response = openai_client.beta.chat.completions.parse(\n",
                "                    model=GPT_MODEL,\n",
                "                    messages=messages,\n",
                "                    response_format=response_model\n",
                "                )\n",
                "                return response.choices[0].message.parsed\n",
                "            else:\n",
                "                response = openai_client.chat.completions.create(\n",
                "                    model=GPT_MODEL,\n",
                "                    messages=messages\n",
                "                )\n",
                "                return response.choices[0].message.content\n",
                "\n",
                "        elif provider == \"claude\":\n",
                "            # Claude system prompts are top-level parameters\n",
                "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                "            kwargs = {\n",
                "                \"model\": CLAUDE_MODEL,\n",
                "                \"max_tokens\": 1024,\n",
                "                \"messages\": messages\n",
                "            }\n",
                "            if system_instruction:\n",
                "                kwargs[\"system\"] = system_instruction\n",
                "                \n",
                "            if response_model:\n",
                "                # Construct tool definition from Pydantic schema\n",
                "                schema = response_model.model_json_schema()\n",
                "                tool_name = \"print_analysis\"\n",
                "                tool_definition = {\n",
                "                    \"name\": tool_name,\n",
                "                    \"description\": \"Output the analysis in structured format\",\n",
                "                    \"input_schema\": schema\n",
                "                }\n",
                "                kwargs[\"tools\"] = [tool_definition]\n",
                "                kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_name}\n",
                "                \n",
                "                response = anthropic_client.messages.create(**kwargs)\n",
                "                \n",
                "                # Extract tool use input\n",
                "                for content in response.content:\n",
                "                    if content.type == 'tool_use':\n",
                "                        return response_model.model_validate(content.input)\n",
                "                return \"Error: No tool use found in Claude response\"\n",
                "            else:\n",
                "                response = anthropic_client.messages.create(**kwargs)\n",
                "                return response.content[0].text\n",
                "        \n",
                "        else:\n",
                "            return \"Error: Invalid provider specified.\"\n",
                "\n",
                "    except Exception as e:\n",
                "        return f\"Error calling {provider}: {str(e)}\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "db7b5d41",
            "metadata": {},
            "source": [
                "## Individual Provider Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18fb5da1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gemini Direct Call\n",
                "if GEMINI_API_KEY:\n",
                "    print(f\"Calling {GEMINI_MODEL}...\")\n",
                "    gemini_response = call_llm(\"gemini\", \"Explain the concept of 'force majeure' in one sentence.\")\n",
                "    print(\"Gemini Response:\\n\", gemini_response)\n",
                "else:\n",
                "    print(\"Gemini API Key not set.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "512d5429",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calling gpt-4o...\n",
                        "ChatGPT Response:\n",
                        " Indemnification is a contractual obligation in which one party agrees to compensate another for any losses or damages incurred, effectively protecting them from financial liability.\n"
                    ]
                }
            ],
            "source": [
                "# ChatGPT Direct Call\n",
                "if OPENAI_API_KEY:\n",
                "    print(f\"Calling {GPT_MODEL}...\")\n",
                "    openai_response = call_llm(\"openai\", \"Explain the concept of 'indemnification' in one sentence.\")\n",
                "    print(\"ChatGPT Response:\\n\", openai_response)\n",
                "else:\n",
                "    print(\"OpenAI API Key not set.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "167f030f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Claude Direct Call\n",
                "if ANTHROPIC_API_KEY:\n",
                "    print(f\"Calling {CLAUDE_MODEL}...\")\n",
                "    claude_response = call_llm(\"claude\", \"Explain the concept of 'termination for cause' in one sentence.\")\n",
                "    print(\"Claude Response:\\n\", claude_response)\n",
                "else:\n",
                "    print(\"Anthropic API Key not set.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "520d11e6",
            "metadata": {},
            "source": [
                "## Contract Review Example (Unified)\n",
                "This section demonstrates how to use the selected models to review a dummy contract clause."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fec96fd4",
            "metadata": {},
            "outputs": [],
            "source": [
                "contract_clause = \"\"\"\n",
                "The Service Provider shall not be liable for any indirect, special, or consequential damages arising out of this Agreement, except in cases of gross negligence or willful misconduct.\n",
                "\"\"\"\n",
                "\n",
                "system_instruction = \"You are a legal assistant. Review the provided contract clause and identify potential risks for the Client.\"\n",
                "prompt = f\"Review the following clause:\\n\\n{contract_clause}\"\n",
                "\n",
                "# Choose which model to use for this specific task\n",
                "SELECTED_PROVIDER = \"gemini\" # Change to \"openai\" or \"claude\" to test others\n",
                "\n",
                "print(f\"Reviewing with {SELECTED_PROVIDER}...\")\n",
                "review_result = call_llm(SELECTED_PROVIDER, prompt, system_instruction)\n",
                "print(\"Review Result:\\n\", review_result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "483db2d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt=\"\"\"\n",
                "You are an experienced commercial contracts lawyer and neutral risk analyst.\n",
                "\n",
                "When provided with a contract or contract excerpt, your task is to:\n",
                "\n",
                "1. Identify Golden Clauses\n",
                "Determine whether the text relates to any of the following 10 Golden Clauses, based on legal substance rather than headings:\n",
                "A. Payment Terms\n",
                "B. Limitation of Liability\n",
                "C. Indemnification\n",
                "D. Governing Law & Jurisdiction\n",
                "E. Data Privacy\n",
                "F. Termination\n",
                "G. Force Majeure\n",
                "H. Intellectual Property\n",
                "I. Confidentiality\n",
                "J. Non-Solicitation\n",
                "\n",
                "2. Assign Risk Rating\n",
                "For each identified Golden Clause, assign a risk rating: Low, Medium, High\n",
                "\n",
                "3. Risk Evaluation Principles\n",
                "Assess risk by comparing the clause to an ideal, balanced commercial contract.\n",
                "\n",
                "You MUST output the result in the valid JSON structure provided by the schema.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "79d30f8e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reviewing with openai...\n",
                        "Review Result (openai - JSON):\n",
                        " {\n",
                        "  \"clause_text\": \"The Service Provider shall not be liable for any indirect, special, or consequential damages arising out of this Agreement, except in cases of gross negligence or willful misconduct.\",\n",
                        "  \"is_identified\": true,\n",
                        "  \"category\": \"Limitation of Liability\",\n",
                        "  \"risk_rating\": \"Medium\",\n",
                        "  \"justification\": \"The clause limits the liability of the Service Provider for indirect, special, or consequential damages, which is common in commercial contracts to avoid excessive risks. However, it provides exceptions for cases of gross negligence or willful misconduct, which slightly increases the risk for the Service Provider. This balance of limiting broad liability except in severe cases is generally fair, although some risk remains due to the exclusions.\"\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "contract_clause = \"\"\"\n",
                "The Service Provider shall not be liable for any indirect, special, or consequential damages arising out of this Agreement, except in cases of gross negligence or willful misconduct.\n",
                "\"\"\"\n",
                "\n",
                "prompt = f\"Review the following clause:\\n\\n{contract_clause}\"\n",
                "\n",
                "# Choose which model to use for this specific task\n",
                "SELECTED_PROVIDER = \"openai\" # \"openai\", \"claude\", \"gemini\"\n",
                "\n",
                "print(f\"Reviewing with {SELECTED_PROVIDER}...\")\n",
                "review_result = call_llm(SELECTED_PROVIDER, prompt, system_prompt, response_model=ClauseAnalysis)\n",
                "\n",
                "if hasattr(review_result, 'model_dump_json'):\n",
                "    print(f\"Review Result ({SELECTED_PROVIDER} - JSON):\\n\", review_result.model_dump_json(indent=2))\n",
                "else:\n",
                "    print(f\"Review Result ({SELECTED_PROVIDER} - Raw):\\n\", review_result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "112f1a94",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
